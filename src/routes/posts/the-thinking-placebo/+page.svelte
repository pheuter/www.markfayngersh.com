<svelte:head>
	<title>Mark Fayngersh | The Thinking Placebo</title>
	<meta
		name="description"
		content="The list of human abilities that computers cannot do is swiftly diminishing, and we may
		want to be wary of how our current epistemology of thinking confounds the progress being
		made and its implications."
	/>
	<meta name="author" content="Mark Fayngersh" />
</svelte:head>

<div class="px-4 py-10 sm:px-6 lg:px-8 lg:py-24">
	<div
		class="prose mx-auto dark:prose-invert lg:prose-lg prose-headings:font-serif prose-headings:font-normal prose-p:font-light prose-a:text-blue-600 prose-a:no-underline hover:prose-a:underline dark:prose-a:text-blue-400"
	>
		<a class="mb-4 inline-flex items-center gap-1" href="/posts">
			<svg
				xmlns="http://www.w3.org/2000/svg"
				viewBox="0 0 20 20"
				fill="currentColor"
				class="h-5 w-5"
			>
				<path
					d="M7 3.5A1.5 1.5 0 018.5 2h3.879a1.5 1.5 0 011.06.44l3.122 3.12A1.5 1.5 0 0117 6.622V12.5a1.5 1.5 0 01-1.5 1.5h-1v-3.379a3 3 0 00-.879-2.121L10.5 5.379A3 3 0 008.379 4.5H7v-1z"
				/>
				<path
					d="M4.5 6A1.5 1.5 0 003 7.5v9A1.5 1.5 0 004.5 18h7a1.5 1.5 0 001.5-1.5v-5.879a1.5 1.5 0 00-.44-1.06L9.44 6.439A1.5 1.5 0 008.378 6H4.5z"
				/>
			</svg>
		</a>
		<h1>The Thinking Placebo</h1>
		<p>
			A lot of skepticism leveled at OpenAI's <a
				href="https://platform.openai.com/docs/models/gpt-3">latest</a
			> (as of writing) text generator focuses on the notion that machines cannot think unless there
			is strong evidence of their producing genuinely new things—while a novel advancement in AI, GPT-3
			is essentially copy-paste (albeit very clever and context-sensitive copy-paste) and ultimately
			does little, if anything, to bridge the gap of cognition.
		</p>
		<p>
			There are many things machines do today that, if they were done by human beings, we might be
			tempted to call thinking. Even something as fundamental as language acquisition can appear on
			the surface to be mere copying of basic structures and rules, i.e. young children tracing
			letters of the alphabet or repeating spoken words, and if we try to dig deeper we run into
			<a href="https://en.wikipedia.org/wiki/Language_acquisition#General_approaches">theories</a>
			that conflict on exactly how and why it is so effective given the sheer combinatorial complexity
			of grammar. Even so, most people would comfortably conclude that children are still thinking throughout
			the process. And yet when we witness a computer program appear to produce the same outputs given
			the same inputs (recently with more success and to greater effect and nuance) we don't hesitate
			to discount that as mere pattern recognition, a mindless and rote execution of algorithms. Is a
			child acquiring language not a form of pattern recognition? Are we confident there is an essential
			difference between the two?
		</p>
		<p>
			One interesting question isn't whether computer programs are actually capable of thinking, but
			rather if what we consider to be thinking is possibly more banal and
			<a href="https://www.edge.org/response-detail/27126">substrate-independent</a>. If we reduce
			our conceptual model of thinking to materialist hardware that processes inputs, forms
			patterns, and generates outputs then the actual hardware should at least in theory become less
			and less interesting as the outputs become more and more robust. This is after all the main
			premise of the Turing Test—the differences between a computer program and a human thinker are
			less important than the ability to differentiate between the human and the program in the
			first place. In fact there are <a href="https://gwern.net/gpt-3">examples</a>
			of GPT-3 applied to these tests and even more creative tasks with compelling results.
		</p>
		<p>
			There remain differences in hardware (brain and silicon) that we need to continue to track. We
			still don't understand human intelligence on a fundamental level, and there are many human
			abilities that machines simply cannot do. But that list is swiftly diminishing, and we may
			want to be wary of how our current epistemology of thinking confounds the progress being made.
		</p>
		<p>
			Perhaps GPT-3 and the class of AI it represents is a kind of placebo, and much like its
			pharmaceutical counterpart who's mechanisms of action are not always fully understood, the
			effects are no less real and possibly just as important.
		</p>
	</div>
</div>
